{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#TikTakToe\n",
        "The famous game solved using some famous machine-learning algorithms:\n",
        "* MiniMax\n",
        "* NeuralNetwork\n",
        "\n",
        "The MiniMax algorithm consists in generating a tree of the possible outcomes of a run. Two players alternate moves, and each set of moves can be translated in a node of the tree.\n",
        "The tree is explored up to the leafs, which are the end states of the game. Considering which player has won at the end, each final leaf is associated with a +1 if the first player won, -1 if the second player won, and 0 if no one is winning.\n",
        "The values are backpropagated up to the root. When the state/node and the player are determined, the algorithm allow to select a winning branch.\n",
        "\n",
        "\n",
        "\n",
        "Convention: the TicTacToe board is composed by 9 cells. The configuration remember the layout of the numeric keypads. Therefore, as convention 1 is the top-left cell, followed by 2 and 3 on its right. 4 is below 1, and 9 is at the bottom right of the board.\n"
      ],
      "metadata": {
        "id": "jdgsfmWT56Sk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, the class Node, representing each state of the game, and the tree are initialized. During initialization, the tree values are not initialized, except for the leafs. Each leafs is associated with a value +1 or -1, based on the winning player."
      ],
      "metadata": {
        "id": "o1BCuOIn7ip0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Cqsk5dMB5RL8"
      },
      "outputs": [],
      "source": [
        "class Node(object):\n",
        "\n",
        "    def __init__(self,name=0, path =[], link=[], value = 100,player= 1,blcklist = [1,2,3,4,5,6,7,8,9],\n",
        "                                                                                         wthlist=[], rlist=[],blist=[]):\n",
        "        self.name= name               # numerical ID of the node\n",
        "        self.path = path              # numerical ID sequences, how we have reached this node from the original root node\n",
        "        self.link = link              # ID of sons nodes\n",
        "        self.value = value\n",
        "        self.player = player\n",
        "        self.blackList = blcklist     # available moves from the current state\n",
        "        self.whiteList = wthlist      # collect the moves to reach the node\n",
        "        self.redList = rlist          # moves of the player red\n",
        "        self.blueList = blist         # moves of player blue"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "# this utility function first retrieve elements not common to 2 lists\n",
        "# then return the first element\n",
        "def get_move(list1,list2):\n",
        "    s = [i for i in list1 if i not in list2]\n",
        "    return s[0]\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# this function will be useful to determine winning state\n",
        "# the input in an array of numbers, corresponding to the positions\n",
        "# of the TicTacToe cells on a board\n",
        "def check_winner(the_list):\n",
        "    greenList = [ [1,2,3], [4,5,6], [7,8,9],[1,4,7], [2,5,8], [3,6,9],[1,5,9], [3,5,7]]\n",
        "    answer = False\n",
        "    for el in greenList:\n",
        "        if el[0] in the_list and el[1] in the_list and el[2] in the_list:\n",
        "            return True\n",
        "    return answer\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "listOfNodes = [ [] , [], [], [], [] ,[] ,[] ,[] ,[] ,[] ]  # this variable will store nodes subdivided by layer ( 0-> init game, 10-> end of the game)\n",
        "keys = dict()                                              # this variable will be useful to store nodes name as key and values (-1,0,1) as values\n",
        "#--------------\n",
        "node0 = Node(name =0, path=[0], player =0)    # first node creation ==> empty node\n",
        "listOfNodes[0].append(node0)\n",
        "#--------------\n",
        "counter =0     # used to give name to new nodes\n",
        "#--------------\n",
        "# main cycle -  tree creation\n",
        "\n",
        "for i in range(len(listOfNodes)):   # 1-> 9\n",
        "    subArray = listOfNodes[i]\n",
        "\n",
        "    for h in range(len(subArray)):\n",
        "        node = subArray[h]\n",
        "\n",
        "        if node.value == 100:    # is the tree already ended? if not, go ahead  (100 is a default value != -1,0,1)\n",
        "            #--------------\n",
        "            # from the array of elements, delete one item at time to have all combinations\n",
        "            list_of_black = []                        # very ugly but there were some problems...\n",
        "            for bl in range(1,len(node.blackList)+1): # it creates an array for many new black lists\n",
        "                x= copy.copy(node.blackList)\n",
        "                del x[bl-1]\n",
        "                list_of_black.append(x)\n",
        "\n",
        "            #--------------\n",
        "            for kk in range(len(list_of_black)):      # now we create parameters for new nodes\n",
        "                new_black = copy.copy(list_of_black[kk])     # an array\n",
        "                mossa = get_move(node.blackList,new_black)   # the difference is the move done\n",
        "                #--------------\n",
        "                counter +=  1\n",
        "                new_name = counter\n",
        "                #--------------\n",
        "                new_path = copy.copy(node.path)\n",
        "                new_path.append(counter)\n",
        "                #--------------\n",
        "                new_white = copy.copy(node.whiteList)\n",
        "                new_white.append(mossa)\n",
        "                #--------------\n",
        "                new_value = 100     # we don't know if a winning node... in case we put different values after\n",
        "\n",
        "                # who is the actual player for that node\n",
        "                player = copy.copy(node.player)\n",
        "                if player == -1 or player == 0:\n",
        "                    new_player = 1\n",
        "                    new_redlist = copy.copy(node.redList)\n",
        "                    new_redlist.append(mossa)\n",
        "                    #--------------\n",
        "                    # check if there is winner\n",
        "                    answer = check_winner(new_redlist)\n",
        "                    if answer== True:\n",
        "                        new_value = 1\n",
        "                        keys[counter] = 1\n",
        "                    new_bluelist = copy.copy(node.blueList)\n",
        "\n",
        "                else:\n",
        "                    new_player = -1\n",
        "                    new_bluelist = copy.copy(node.blueList)\n",
        "                    new_bluelist.append(mossa)\n",
        "                    #--------------\n",
        "                    # check if there is winner\n",
        "                    answer = check_winner(new_bluelist)\n",
        "                    if answer== True:\n",
        "                        new_value =-1\n",
        "                        keys[counter] = -1     # <== the dictionary   ['node_number': 'value']\n",
        "                    new_redlist = copy.copy(node.redList)\n",
        "\n",
        "                # we don't have the winner, moves are ended\n",
        "                if  len(new_black )==0 and answer==False:\n",
        "                    new_value =0\n",
        "                    keys[counter] = 0\n",
        "\n",
        "                # finally create the node with new numbers calculated\n",
        "                new_node = Node(name = new_name,path=new_path,link=[],value = new_value,\n",
        "                                player= new_player,blcklist =new_black, wthlist=new_white,\n",
        "                                                    rlist=new_redlist,blist=new_bluelist )\n",
        "                node.link.append(counter)   # update 'old' node with the son node\n",
        "                listOfNodes[i+1].append(new_node)\n",
        "\n",
        "print(\"tree initialized\")\n",
        "# at the conclusion we have our tree with all values =100 except for final nodes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGu-1lPb915A",
        "outputId": "95e840a5-3780-448d-922f-aa9aed8be309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tree initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the tree is built, we need a decision-rule. This rule consists in deciding which move select, aka which branch we have to follow.\n",
        "\n",
        "\n",
        "Backpropagating the values from the leafs, we can associate a winning branch for all the nodes.\n",
        "In particular, if a node belongs to \"max\" player, we will select among the \"sons nodes\" the one which correspond to the maximum value (+1).\n",
        "On the contrary, if this node belongs to \"min\" player, we will select the node among \"sons nodes\" which corresponds to the minimum value (-1).\n",
        "\n",
        "Below, starting from the leafs, the value are backpropagated to the root."
      ],
      "metadata": {
        "id": "zFjjqQI0-VSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_Value( value_list, player_coeff):\n",
        "    # from a list of values (-1,0,1), retrieve the maximum value\n",
        "    # considering the \"color\" of the player, -1 or 1\n",
        "    if player_coeff < 0:\n",
        "      min_val = min(value_list)\n",
        "      min_pos = value_list.index(min_val)\n",
        "      return [min_val, min_pos]\n",
        "    else:\n",
        "      max_val = max(value_list)\n",
        "      max_pos = value_list.index(max_val)\n",
        "      return [max_val, max_pos]\n",
        "\n",
        "\n",
        "\n",
        "#------------------------\n",
        "# MINIMAX ALGO\n",
        "indexes = [n for n in range(0,10)] [::-1]    # start from the last layer\n",
        "\n",
        "for i in indexes:\n",
        "  print(\"updating layer \",i,\"/9\")\n",
        "  subArray = listOfNodes[i]   # nodes in the right-most\n",
        "\n",
        "  for n in range(len(subArray)):\n",
        "      node = subArray[n]      # a node\n",
        "      #------------------------\n",
        "      if len(node.link)>0 :  # if it has sons\n",
        "          player = copy.copy(node.player)\n",
        "          if player ==0:\n",
        "              player = 1\n",
        "\n",
        "          # now create for the node array for link and values of corresponding nodes\n",
        "          sons = []\n",
        "          values_of_sons=[]\n",
        "\n",
        "          for l in range(len(node.link)):\n",
        "              sons.append(node.link[l])\n",
        "              son_value = keys.get(node.link[l]) # the value gathered from the leafs\n",
        "              values_of_sons.append(son_value)\n",
        "\n",
        "          values = find_Value(values_of_sons,(-1)*player)\n",
        "          node.value = values[0]\n",
        "          node.link =[sons[values[1]]]\n",
        "          keys[node.path[-1]] = values[0]  # update the value of this node\n",
        "print(\"tree updated for all players\")"
      ],
      "metadata": {
        "id": "svPoV8hj917Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can play.   \n",
        "The player will have first to select if it is starting, or if \"machine\" will move fist.   \n",
        "The game accepts numerical inputs in the range 1-9, which corresponds to the cells of the board."
      ],
      "metadata": {
        "id": "Rfa2ps9PAjHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can start with the game\n",
        "\n",
        "def find_node(subArray, WhiteList):\n",
        "    for i in range(len(subArray)):\n",
        "        node = subArray[i]\n",
        "\n",
        "        if node.whiteList== WhiteList:\n",
        "            return i\n",
        "    return -1\n",
        "\n",
        "def next_node(subArray, Name):\n",
        "    for i in range(len(subArray)):\n",
        "        node = subArray[i]\n",
        "        if node.name == Name:\n",
        "            return node\n",
        "    return False\n",
        "\n",
        "\n",
        "#-------------------------\n",
        "def human_turn(whiteList):\n",
        "    #n= n+1\n",
        "    human = int(input(\"give your next move: \"))\n",
        "    if human in whiteList:\n",
        "        if len(whiteList)<1:\n",
        "            #print(\"game ended\")\n",
        "            return False\n",
        "        else:\n",
        "            print(\"invalid key\")\n",
        "        return False\n",
        "    else:\n",
        "        next_move = human\n",
        "    return next_move\n",
        "#---------------------------\n",
        "def machine_turn(n,listOfNodes, whiteList):\n",
        "    subArray = copy.copy(listOfNodes[n])\n",
        "    position = find_node(subArray, whiteList)\n",
        "    if position == -1:\n",
        "        return False\n",
        "    else:\n",
        "        node = subArray[position]\n",
        "        if len(node.link)>0:\n",
        "            next_link = node.link[0]\n",
        "            new_node_name = next_link\n",
        "            new_node = next_node(listOfNodes[n+1], new_node_name)\n",
        "            next_move = get_move(node.blackList,new_node.blackList)\n",
        "            return next_move\n",
        "        #else:\n",
        "        #    print(\"game ended\")\n",
        "    return True\n",
        "\n",
        "\n",
        "#--------------------------\n",
        "\n",
        "def printBoard(board):\n",
        "    print(\"\\n\")\n",
        "    print(board[\"1\"] + ' | ' + board[\"2\"] + ' | ' + board[\"3\"])\n",
        "    print('-- + -- + --')\n",
        "    print(board[\"4\"] + ' | ' + board[\"5\"] + ' | ' + board[\"6\"])\n",
        "    print('-- + -- + --')\n",
        "    print(board[\"7\"] + ' | ' + board[\"8\"] + ' | ' + board[\"9\"])\n",
        "\n",
        "#--------------------------\n",
        "\n",
        "def Game(starting_machine, listOfNodes):\n",
        "    board = {\"7\": '  ' , \"8\": '  ' , \"9\": '  ' ,\"4\": '  ' , \"5\": '  ' , \"6\": '  ' ,\"1\": '  ' , \"2\": '  ' , \"3\": '  ' }\n",
        "\n",
        "    whiteList = []\n",
        "    moves_human = [  ]\n",
        "    moves_machine = [  ]\n",
        "    #-------------------------\n",
        "    if starting_machine == \"no\":\n",
        "        n = -1\n",
        "        for j in range(0,5):\n",
        "\n",
        "            n = n+1\n",
        "            next_move = human_turn(whiteList)\n",
        "            if next_move == False:\n",
        "                return False\n",
        "            whiteList.append(next_move)\n",
        "            board[str(next_move)] = \" X\"\n",
        "            moves_human.append(next_move)\n",
        "\n",
        "            if check_winner(moves_human):\n",
        "              print(\"game ended  - human winner !\")\n",
        "              printBoard(board)\n",
        "              return True\n",
        "            printBoard(board)\n",
        "            #---------------------------\n",
        "            n = n+1\n",
        "            next_move = machine_turn(n,listOfNodes, whiteList)\n",
        "            whiteList.append(next_move)\n",
        "            board[str(next_move)] = \" O\"\n",
        "            moves_machine.append(next_move)\n",
        "            printBoard(board)\n",
        "\n",
        "            if check_winner(moves_machine):\n",
        "              print(\"game ended  - machine winner !\")\n",
        "              return True\n",
        "\n",
        "        return True\n",
        "    #-------------------------\n",
        "    else:\n",
        "        n = -1\n",
        "        for j in range(0,5):\n",
        "            n = n+1\n",
        "            next_move = machine_turn(n,listOfNodes, whiteList)\n",
        "            whiteList.append(next_move)\n",
        "            board[str(next_move)] = \" O\"\n",
        "            moves_machine.append(next_move)\n",
        "            printBoard(board)\n",
        "\n",
        "            if check_winner(moves_machine):\n",
        "              print(\"game ended  - machine winner !\")\n",
        "              return True\n",
        "            #---------------------------\n",
        "            n = n+1\n",
        "            next_move = human_turn(whiteList)\n",
        "            whiteList.append(next_move)\n",
        "            board[str(next_move)] = \" X\"\n",
        "            moves_human.append(next_move)\n",
        "\n",
        "            if check_winner(moves_human):\n",
        "              print(\"game ended  - human winner !\")\n",
        "              printBoard(board)\n",
        "              return True\n",
        "        return True\n",
        "    print(\"\\n\")\n",
        "\n",
        "#machine = bool(input(\"is machine starting? --   \\nTrue if yes \\nFalse if not\\n\"))\n",
        "machine =  input(\"is machine starting? --   \\nyes \\nno\\n\")\n",
        "print(\"you are 'X', machine is 'O'\")\n",
        "Game(machine,listOfNodes)"
      ],
      "metadata": {
        "id": "9SiZNdp5919v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Networks are wide used in a vast range of different problems. As well, Neural Networks are used in Video Games to enable autonomous reasoning.   \n",
        "\n",
        "The next session will use the famous Pytorch library (https://pytorch.org/) to train a Multi Layer Perceptron able to play the game.\n",
        "The first problem is how we get the training data. Once we have our MiniMax algorithm, we let a MiniMax agent play against another automatic agent to generate a dataset. Each entry of the dataset has as many features as the number of cells of the board. Each feature corresponds to a cell, and its value is:  \n",
        "* +1 if the machine1 made the move in that cell  \n",
        "* -1 if the machine2 made the move in that cell\n",
        "* 0 if it is still available\n",
        "\n",
        "The targets are the desired output of our network. The targets are coded in the same way as the feature, but the values are:\n",
        "* 1 if that cell is our desired move\n",
        "* 0 othervise\n",
        "\n",
        "once we can associate a board-state before the agent's move and its corresponding best move, we can train the network.   \n",
        "\n",
        "The following snippet is a battle between two machines to collect data. Because we need to explore the solution space, one machine follows the exact MiniMax algo, while the other is subject to randomness. This is like a Monte Carlo Simulation."
      ],
      "metadata": {
        "id": "BiqbSCC0BSjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import itertools\n",
        "\n",
        "\n",
        "# generate dataset for the the ANN training\n",
        "starting_machine =1\n",
        "number_rounds = 1000\n",
        "random_probability = 0.85\n",
        "\n",
        "\n",
        "def generate_dataset(number_rounds, random_probability):\n",
        "  dataset = []    # each entry will be 9 position for board state + 9 final number marking desired position as one -hot encoding\n",
        "                  # to be returned\n",
        "  # to print advance\n",
        "  for round in range(number_rounds):\n",
        "    if round%100 == 0:\n",
        "      print(\"round: \" + str(round))\n",
        "\n",
        "    # play different games machine vs machine\n",
        "    whiteList = []\n",
        "    moves_machine1 = [  ]\n",
        "    moves_machine2 = [  ]\n",
        "    all_moves = [ i for i in range(1, 10)]\n",
        "    board_state = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "    #print( \"new game\" )\n",
        "    n = -1\n",
        "    for j in range(0,5):\n",
        "\n",
        "      n = n+1\n",
        "      # machine 2 sometimes do wrong\n",
        "      rand = random.uniform(0, 1)\n",
        "      if ( rand < random_probability):\n",
        "        available_moves = [x for x in all_moves if x not in whiteList]\n",
        "        # random move\n",
        "        next_move = random.choice(available_moves)\n",
        "      else:\n",
        "        next_move = machine_turn(n,listOfNodes, whiteList)\n",
        "      #print(next_move)\n",
        "      whiteList.append(next_move)\n",
        "      moves_machine2.append(next_move)\n",
        "      board_state[next_move-1] = -1\n",
        "      #print(board_state )\n",
        "      if check_winner(moves_machine2) == True:\n",
        "          j = 1000\n",
        "          continue\n",
        "      elif len(whiteList) >= 9:\n",
        "          j = 1000\n",
        "          continue\n",
        "\n",
        "      n = n+1\n",
        "      # machine 1 makes best move always\n",
        "      next_move = machine_turn(n,listOfNodes, whiteList)\n",
        "      whiteList.append(next_move)\n",
        "      moves_machine1.append(next_move)\n",
        "\n",
        "      new_data = [ b for b in board_state]  # board before new move\n",
        "      target = [ 0 for _ in range(1,10)]\n",
        "      target[next_move-1] = 1\n",
        "      dataset.append( new_data + target)\n",
        "\n",
        "      board_state[next_move-1] = 1           # new board state\n",
        "\n",
        "      #print(board_state )\n",
        "      if check_winner(moves_machine1) == True:\n",
        "          j = 1000\n",
        "          break\n",
        "      elif len(whiteList) >= 9:\n",
        "          j = 1000\n",
        "          break\n",
        "  return dataset\n",
        "\n",
        "# a dataset with all the generated runs and outcomes\n",
        "dataset = generate_dataset(number_rounds, random_probability)\n",
        "\n",
        "# remove duplicates, to not bias the training that will follow\n",
        "dataset.sort()\n",
        "clean_dataset = list(dataset for dataset,_ in itertools.groupby(dataset))   # remove duplicate\n",
        "\n",
        "# based on currently available outcomes, because the two machines have been playing always\n",
        "# in the same side, we generate a specular array as if the two machine would have swapped side\n",
        "specular = []\n",
        "maxLen = len(clean_dataset)\n",
        "for i in range(maxLen):\n",
        "  data_entry = clean_dataset[i]\n",
        "  features = [ -1*x for x in data_entry[0:9]]\n",
        "  targs = [ t for t in data_entry[9:18] ]\n",
        "  specular_array = features + targs\n",
        "  clean_dataset.append(specular_array)\n",
        "\n",
        "\n",
        "print(\"dataset size: \" + str(len(clean_dataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3sq0uuo91_4",
        "outputId": "1f655aa6-dc61-42a9-a6e0-31a4c8aaf924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "round: 0\n",
            "round: 100\n",
            "round: 200\n",
            "round: 300\n",
            "round: 400\n",
            "round: 500\n",
            "round: 600\n",
            "round: 700\n",
            "round: 800\n",
            "round: 900\n",
            "dataset size: 804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we initialize and train the Neural Network."
      ],
      "metadata": {
        "id": "lrr1RAsDEMWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class ANN(nn.Module):\n",
        "    def __init__(self, in_size, hidden_size, out_size):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear( in_size, hidden_size)\n",
        "        self.linear2 = nn.Linear( hidden_size, hidden_size)\n",
        "        self.outLayer = nn.Linear( hidden_size, out_size)\n",
        "        self._init_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "          x = torch.relu(self.linear1(x))\n",
        "          x = torch.relu(self.linear2(x))\n",
        "          x = torch.sigmoid(self.outLayer(x))\n",
        "          return x\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # initialize weights in proper way\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "\n",
        "# initialize the features and the targets(labels)\n",
        "clean = np.array( [  np.array(cl) for cl in clean_dataset])\n",
        "\n",
        "only_features = clean[:,0:9]\n",
        "only_targets = clean[:,9:18]\n",
        "targets = torch.tensor(only_targets, dtype=torch.float32)\n",
        "features = torch.tensor(only_features, dtype=torch.float32)\n",
        "\n",
        "model = ANN(in_size= 9, hidden_size=64, out_size= 9 )\n",
        "print(model)\n",
        "\n",
        "\n",
        "# loss and optimization\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# batch handler\n",
        "loader = DataLoader(TensorDataset(features,targets), batch_size=10, shuffle=True)\n",
        "\n",
        "\n",
        "# training loop\n",
        "epoch_number = 300\n",
        "for epoch in range(epoch_number):\n",
        "\n",
        "  for X_batch, y_batch in loader:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_batch)\n",
        "    loss = criterion(outputs, y_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "8C9u9EjPlB7_",
        "outputId": "a45b8d80-89ee-478b-fcce-b626020e422f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANN(\n",
            "  (linear1): Linear(in_features=9, out_features=64, bias=True)\n",
            "  (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (outLayer): Linear(in_features=64, out_features=9, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ncount = 0\\nfor d in range(len(features) ):\\n  prediction = model.forward(features[d])\\n\\n  max_pred = max(prediction)\\n  b = prediction == max_pred\\n  max_pos_pred = b.nonzero()\\n\\n\\n  max_tar = max(targets[d])\\n  c = targets[d] == max_tar\\n  max_pos_tar = c.nonzero()\\n\\n  if max_pos_tar == max_pos_pred:\\n    count +=1\\n\\nprint(count/ len(features))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now we can play. Our agent should be trained at least to defeat us..."
      ],
      "metadata": {
        "id": "lbQdAK-2GDCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play_vs_ANN(network):\n",
        "\n",
        "    # retrieve the ANN's move, that must be feasible\n",
        "    def decide_move( wlist, network_outputs):\n",
        "      for w in reversed(wlist):\n",
        "        network_outputs[w-1] = -100000000000      # cannot be chosen\n",
        "\n",
        "      maxval = max(network_outputs )\n",
        "      max_pos = network_outputs.index(maxval)\n",
        "      return max_pos+1\n",
        "    # ----------------------------------------------\n",
        "\n",
        "    board = {\"7\": '  ' , \"8\": '  ' , \"9\": '  ' ,\"4\": '  ' , \"5\": '  ' , \"6\": '  ' ,\"1\": '  ' , \"2\": '  ' , \"3\": '  ' }\n",
        "\n",
        "    whiteList = []\n",
        "    moves_human = []\n",
        "    moves_machine = []\n",
        "    board_state = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "    n = -1\n",
        "    for j in range(0,5):\n",
        "        n = n+1\n",
        "\n",
        "        outputs = network.forward(torch.tensor(board_state, dtype=torch.float32))\n",
        "\n",
        "        next_move = decide_move(whiteList, outputs.tolist() )\n",
        "        whiteList.append(next_move)\n",
        "        board_state[next_move-1] = 1\n",
        "\n",
        "        board[str(next_move)] = \" O\"\n",
        "        moves_machine.append(next_move)\n",
        "        printBoard(board)\n",
        "\n",
        "        if check_winner(moves_machine):\n",
        "          print(\"game ended  - machine winner !\")\n",
        "          return True\n",
        "        #---------------------------\n",
        "        n = n+1\n",
        "        next_move = human_turn(whiteList)\n",
        "        whiteList.append(next_move)\n",
        "        board_state[next_move-1] = -1\n",
        "\n",
        "        board[str(next_move)] = \" X\"\n",
        "        moves_human.append(next_move)\n",
        "\n",
        "        if check_winner(moves_human):\n",
        "          print(\"game ended  - human winner !\")\n",
        "          printBoard(board)\n",
        "          return True\n",
        "\n",
        "        if len(whiteList) >= 9:\n",
        "          print(\"no winner\")\n",
        "          return True\n",
        "\n",
        "play_vs_ANN(model)\n"
      ],
      "metadata": {
        "id": "D790mB5nxFeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Neural Network can also be trained to approximate the Q-table. In reinforcement learning, the Q-table is used by autonomous agents to take decisions. The table correlate a state, such as a board state in the tic-tac-toe game, and an action, such as what is the next move.    \n",
        "A Neural Network can be used in place of the table.  \n",
        "\n",
        "Next, a new Neural Network is defined."
      ],
      "metadata": {
        "id": "noz77SZNsaHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class QNN(nn.Module):\n",
        "    def __init__(self, in_size=9, hidden_size=128, out_size=9):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(in_size, hidden_size)\n",
        "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer4 = nn.Linear(hidden_size, out_size)\n",
        "        #self.activation = nn.LeakyReLU(0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.layer1(x))\n",
        "        x = F.leaky_relu(self.layer2(x))\n",
        "        x = F.leaky_relu(self.layer3(x))\n",
        "        #x = self.activation(self.layer1(x))\n",
        "        #x = self.activation(self.layer2(x))\n",
        "        x = self.layer4(x)  # nessuna attivazione qui\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "_bhMRYH5GZXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train the Network, the agent is initialized with a default model. The agent can decide, but it is not trained to play effectively.  \n",
        "Training requires \"experience\", such as games in which the agent collects rewards (if wins) or penalties (if looses).  \n",
        "A proper training is structured to offer a good set of experience to the agent, also know as exploration vs esploration trade-off.   \n",
        "In the code below, esperiences are collected playing the game, and stored in a replayBuffer object. Time by time, the agent look back in ints memory and updates its status."
      ],
      "metadata": {
        "id": "ExgueZFAt1vY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# data format to generate a state of this run\n",
        "def generate_memory_entry(board_state, next_move, reward, is_end_game, one ):    # next_move gi√† normalizzato 0 - 8\n",
        "        new_memory = np.array( [0 for _ in range(21)] )\n",
        "        new_memory[0:9] = np.array(board_state)     # current state of the board\n",
        "        new_memory[9:18] = np.array(board_state)    # this line and the one below  update the board to next state\n",
        "        new_memory[9 + next_move] = one             # 1 or -1 on the basis of the player\n",
        "        new_memory[18] = next_move          # move coded as  0 - 8\n",
        "        new_memory[19] = reward\n",
        "        new_memory[20] = is_end_game        # only if win, loose or draw\n",
        "        return new_memory\n",
        "\n",
        "\n",
        "def Qtraining( Qmodel, number_rounds, greedy_probability):\n",
        "\n",
        "  # data format to store states of the game\n",
        "  memory_states = np.zeros(shape=(1, 21))\n",
        "\n",
        "  # rewards if win, loss, nothing happens or draw\n",
        "  r_win =  10\n",
        "  r_loss =  -1\n",
        "  r_null = -0.01\n",
        "  r_draw = 0.01\n",
        "\n",
        "  for round in range(number_rounds):   # play the game\n",
        "\n",
        "    whiteList = []\n",
        "    moves_machine1 = [  ]\n",
        "    moves_machine2 = [  ]\n",
        "    all_moves = [ i for i in range(1, 10)]\n",
        "    board_state = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "    # ---\n",
        "\n",
        "    conclude_experience = 0   # 1 if run must end\n",
        "    reward = 0                # according if win, lost or invalid move\n",
        "    n = -1\n",
        "    for j in range(0,5):\n",
        "\n",
        "      n = n+1\n",
        "      # ------------------------------------------------------------------------\n",
        "      # machine 1 is our agent to be trained  ----------------------------------\n",
        "      rand = random.uniform(0, 1)\n",
        "      if ( rand < greedy_probability):    # explore new solutions\n",
        "        available_moves = [x for x in all_moves if x not in whiteList]\n",
        "        next_move = random.choice(available_moves) -1\n",
        "      else:                               # choose action with the network\n",
        "        out = Qmodel.forward( torch.tensor( np.array(board_state)+2 , dtype=torch.float32) )\n",
        "        next_move = torch.argmax(out)     # 0 - 8\n",
        "\n",
        "      # check if it is a valid move\n",
        "      if next_move in  whiteList:\n",
        "          conclude_experience = 1\n",
        "          reward = r_loss\n",
        "          new_memory = generate_memory_entry(board_state, next_move, reward, conclude_experience,1 ) # save the status\n",
        "      else:\n",
        "          whiteList.append(next_move+1)\n",
        "          moves_machine1.append(next_move+1)\n",
        "\n",
        "          if check_winner(moves_machine1) == True:\n",
        "            conclude_experience = 1\n",
        "            reward = r_win\n",
        "            new_memory = generate_memory_entry(board_state, next_move, reward, conclude_experience,1 )\n",
        "          elif len(whiteList) >= 9:\n",
        "            conclude_experience = 1\n",
        "            reward = r_draw\n",
        "            new_memory = generate_memory_entry(board_state, next_move, reward, conclude_experience,1 )\n",
        "\n",
        "          # save an intermediate state\n",
        "          if conclude_experience ==0:\n",
        "            reward = r_null\n",
        "            new_memory = generate_memory_entry(board_state, next_move, reward, conclude_experience,1 )\n",
        "\n",
        "\n",
        "          # update the board\n",
        "          board_state[next_move] = 1\n",
        "\n",
        "      memory_states = np.vstack((memory_states, new_memory))\n",
        "      if conclude_experience ==1:\n",
        "          j=1000   # conclude this run\n",
        "          break\n",
        "\n",
        "\n",
        "\n",
        "      # machine 2 playes randomly or with minimax ------------------------------\n",
        "      if rand < 0.3:      # rand\n",
        "        available_moves = [x  for x in all_moves if x not in whiteList]\n",
        "        next_move = random.choice(available_moves)\n",
        "      else:               # minmax\n",
        "        next_move = machine_turn(n,listOfNodes, whiteList)+1\n",
        "\n",
        "      whiteList.append(next_move)\n",
        "      moves_machine2.append(next_move)\n",
        "\n",
        "      if check_winner(moves_machine2) == True:\n",
        "          conclude_experience = 1\n",
        "          reward = r_loss\n",
        "          new_memory = generate_memory_entry(board_state, next_move-1, reward, conclude_experience,-1 )\n",
        "      elif len(whiteList) >= 9:\n",
        "          conclude_experience = 1\n",
        "          reward = r_draw\n",
        "          new_memory = generate_memory_entry(board_state, next_move-1, reward, conclude_experience,-1 )\n",
        "\n",
        "\n",
        "      if conclude_experience ==0:\n",
        "        reward = r_null\n",
        "        new_memory = generate_memory_entry(board_state, next_move-1, reward, conclude_experience,-1 )\n",
        "\n",
        "      # update board state\n",
        "      board_state[next_move-1] = -1           # new board state\n",
        "\n",
        "      memory_states = np.vstack((memory_states, new_memory))  # stack experiences\n",
        "      if conclude_experience ==1:\n",
        "        j=1000          # conclude this run\n",
        "        break\n",
        "\n",
        "  flipped = memory_states[::-1, :].copy()\n",
        "  return flipped\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# buffer to store played staes\n",
        "class replayBuffer():\n",
        "  def __init__(self, size_buffer):\n",
        "      self.buffer = np.zeros(shape=(size_buffer, 21))\n",
        "      self.index = 0\n",
        "      self.size_buffer = size_buffer\n",
        "      self.is_full = 0\n",
        "\n",
        "  def clear_all(self):\n",
        "      for _ in range(self.size_buffer):\n",
        "        self.buffer[_,:] =  np.array( [0 for _ in range(21)])\n",
        "      self.index = 0\n",
        "      self.is_full = 0\n",
        "\n",
        "\n",
        "  def push(self, entries, dim):  # just because is not a real stack !\n",
        "      # entries is an array, dim its dimension\n",
        "      start_index = self.index\n",
        "      end_index = self.index + dim\n",
        "      delta = dim\n",
        "      if end_index > self.size_buffer:\n",
        "        delta = self.size_buffer - start_index\n",
        "        end_index = self.size_buffer\n",
        "        # update\n",
        "        self.buffer[start_index:end_index, :] = entries[0:delta,:].copy()\n",
        "\n",
        "        # beginning\n",
        "        diff = dim - delta   # difference original and already pushed\n",
        "        self.index = diff\n",
        "        self.buffer[0:self.index, :] = entries[delta:delta+dim,:].copy()\n",
        "        self.is_full = 1\n",
        "      else:\n",
        "        self.buffer[start_index:end_index, :] = entries[0:dim,:].copy()\n",
        "        self.index  = end_index\n",
        "\n",
        "\n",
        "  def random_pop(self, batch_size):\n",
        "      if self.is_full ==1:\n",
        "        idx = np.random.choice(self.size_buffer , batch_size, replace=False)\n",
        "      else:\n",
        "        idx = np.random.choice( self.index , batch_size, replace=False)\n",
        "\n",
        "      nbatch = self.buffer[idx].copy()\n",
        "      return nbatch\n",
        " # -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model definition\n",
        "Qmodel = QNN(in_size= 9, hidden_size= 128, out_size= 9 )   #64\n",
        "#criterion = nn.MSELoss()\n",
        "criterion = nn.SmoothL1Loss()\n",
        "optimizer = optim.Adam(Qmodel.parameters(), lr=0.0001) #0.001)\n",
        "\n",
        "# exploration vs esploitation - greedy\n",
        "greedy_probability_start = 1\n",
        "greedy_probability_end = 0.1\n",
        "\n",
        "# for Q learning\n",
        "discount = 0.95\n",
        "# initialize breplay bbuffer\n",
        "# size buffer to store results\n",
        "size_buffer =  7500\n",
        "replay_buffer = replayBuffer(size_buffer)\n",
        "\n",
        "\n",
        "\n",
        "# at each training step, retrieve a batch\n",
        "batch_size =  100\n",
        "batch = np.zeros(shape=(batch_size, 21))\n",
        "\n",
        "\n",
        "# number of episodes\n",
        "episodes = 7500\n",
        "\n",
        "greedy_probability = greedy_probability_start\n",
        "delta_greedy = (greedy_probability_start - greedy_probability_end)/ (episodes-5000)\n",
        "\n",
        "lst = []\n",
        "loss_lst = []\n",
        "\n",
        "\n",
        "# the target net is used in training as substitute of the policy net\n",
        "target_net = copy.deepcopy(Qmodel)\n",
        "update_tik = 100      # 150                # update rate\n",
        "\n",
        "\n",
        "for i in range(episodes):     # training begins\n",
        "\n",
        "  greedy_probability -= delta_greedy   # update greedy rule\n",
        "  if greedy_probability < greedy_probability_end:\n",
        "    greedy_probability = greedy_probability_end\n",
        "\n",
        "  new_memory_state  = Qtraining( Qmodel, 1,  greedy_probability  ) # play one time\n",
        "  replay_buffer.push(new_memory_state , 1)\n",
        "\n",
        "  if replay_buffer.is_full == 1 or replay_buffer.index > 500:     # when the buffer is ready to be used\n",
        "        batch  = replay_buffer.random_pop(batch_size)             # extract mini-batches\n",
        "\n",
        "        n_states = len(batch)\n",
        "        current_states = torch.tensor(batch[:,0:9] +2 , dtype=torch.float32)      #  <--- [-1,0,1] -> [0,1,2]\n",
        "        next_states = torch.tensor(batch[:,9:18] +2 , dtype=torch.float32)\n",
        "        actions     = torch.tensor(batch[:, 18], dtype=torch.long).unsqueeze(1)\n",
        "        rewards     = torch.tensor(batch[:,19], dtype=torch.float32).unsqueeze(1)\n",
        "        ended       = torch.tensor(batch[:,20], dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "        # compute current Q values\n",
        "        q_values = Qmodel(current_states).gather(1, actions).squeeze()\n",
        "        # compute future Q values\n",
        "        with torch.no_grad():\n",
        "            next_q_values = target_net.forward(next_states).max(1)[0]\n",
        "\n",
        "        rewards = rewards.squeeze()\n",
        "        ended = ended.squeeze()\n",
        "        target_q = rewards + discount * next_q_values * (1 - ended)\n",
        "\n",
        "        loss =  criterion(q_values, target_q)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i%update_tik == 0:\n",
        "          target_net.load_state_dict(Qmodel.state_dict())\n",
        "\n"
      ],
      "metadata": {
        "id": "rjEXMpNlIDbi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}